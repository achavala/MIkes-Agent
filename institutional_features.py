"""
ðŸ¦ INSTITUTIONAL-GRADE FEATURE ENGINEERING MODULE

This module provides Citadel/Jane Street-level feature extraction:
- Multi-timescale features (1m, 5m, 15m, 1h)
- Technical indicators (50+ indicators)
- Volatility features (RV, IV, skew, term structure)
- Market microstructure features
- Cross-asset signals
- Target: 500+ features per observation

Author: Mike Agent Institutional Upgrade
"""

import pandas as pd
import numpy as np
from typing import Dict, Optional, Tuple
import yfinance as yf
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

try:
    from ta import add_all_ta_features
    TA_AVAILABLE = True
except ImportError:
    TA_AVAILABLE = False
    print("âš ï¸ 'ta' library not available. Install with: pip install ta")

try:
    import talib
    TALIB_AVAILABLE = True
except ImportError:
    TALIB_AVAILABLE = False
    print("âš ï¸ 'talib' library not available. Install with: pip install ta-lib")


class InstitutionalFeatureEngine:
    """
    Professional-grade feature engineering for 0DTE trading
    
    Transforms raw OHLCV data into 500+ predictive features:
    - Price-based features (momentum, trend, mean reversion)
    - Volatility features (realized, implied, skew)
    - Volume features (order flow, volume profile)
    - Cross-asset features (VIX, SPX, correlations)
    - Microstructure features (spread, depth, toxicity)
    """
    
    def __init__(self, lookback_minutes: int = 20):
        """
        Initialize feature engine
        
        Args:
            lookback_minutes: Number of minutes to look back for features
        """
        self.lookback_minutes = lookback_minutes
        self.feature_names = []
        self.scalers = {}
        
    def extract_all_features(
        self, 
        data: pd.DataFrame,
        symbol: str = 'SPY',
        risk_mgr=None,
        include_microstructure: bool = True
    ) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:
        """
        Extract all institutional-grade features from market data
        
        Args:
            data: DataFrame with OHLCV data (1-minute bars)
            symbol: Trading symbol (SPY, QQQ, SPX)
            risk_mgr: RiskManager instance (for position info)
            include_microstructure: Whether to include microstructure features
            
        Returns:
            Tuple of:
            - Full feature vector (n_samples, n_features)
            - Dictionary of feature groups for analysis
        """
        if len(data) < 20:
            # Return zeros if not enough data
            empty_features = np.zeros((len(data), 500))
            return empty_features, {}
        
        # Ensure columns are lowercase
        data = data.copy()
        data.columns = [col.lower() for col in data.columns]
        
        # Ensure we have required columns
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in required_cols:
            if col not in data.columns:
                raise ValueError(f"Missing required column: {col}")
        
        feature_groups = {}
        
        # 1. PRICE-BASED FEATURES (50+ features)
        feature_groups['price'] = self._extract_price_features(data)
        
        # 2. VOLATILITY FEATURES (100+ features)
        feature_groups['volatility'] = self._extract_volatility_features(data, symbol)
        
        # 3. VOLUME FEATURES (50+ features)
        feature_groups['volume'] = self._extract_volume_features(data)
        
        # 4. TECHNICAL INDICATORS (150+ features)
        feature_groups['technical'] = self._extract_technical_indicators(data)
        
        # 5. MULTI-TIMESCALE FEATURES (100+ features)
        feature_groups['multi_timescale'] = self._extract_multi_timescale_features(data)
        
        # 6. CROSS-ASSET FEATURES (50+ features)
        feature_groups['cross_asset'] = self._extract_cross_asset_features(symbol, data)
        
        # 7. MARKET MICROSTRUCTURE (50+ features) - if enabled
        if include_microstructure:
            feature_groups['microstructure'] = self._extract_microstructure_features(data)
        else:
            feature_groups['microstructure'] = np.zeros((len(data), 50))
        
        # 8. POSITION & RISK FEATURES (10+ features)
        if risk_mgr:
            feature_groups['position'] = self._extract_position_features(data, risk_mgr)
        else:
            feature_groups['position'] = np.zeros((len(data), 10))
        
        # Combine all feature groups
        all_features = np.hstack([
            feature_groups['price'],
            feature_groups['volatility'],
            feature_groups['volume'],
            feature_groups['technical'],
            feature_groups['multi_timescale'],
            feature_groups['cross_asset'],
            feature_groups['microstructure'],
            feature_groups['position']
        ])
        
        # Handle NaN/Inf values
        all_features = np.nan_to_num(all_features, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # Store feature names for debugging
        self.feature_names = [f'feature_{i}' for i in range(all_features.shape[1])]
        
        return all_features, feature_groups
    
    def _extract_price_features(self, data: pd.DataFrame) -> np.ndarray:
        """Extract 50+ price-based features"""
        features = []
        
        close = data['close'].values
        high = data['high'].values
        low = data['low'].values
        open_price = data['open'].values
        
        # Basic price transforms
        features.append(close / close[0] - 1)  # Normalized return from start
        features.append(np.log(close / close[0]))  # Log return
        
        # Returns (multiple timeframes)
        for period in [1, 2, 3, 5, 10, 20]:
            if len(close) > period:
                returns = np.diff(close, period) / close[:-period]
                returns = np.pad(returns, (period, 0), 'constant', constant_values=0)
                features.append(returns)
            else:
                features.append(np.zeros(len(close)))
        
        # Momentum features
        for period in [5, 10, 20]:
            if len(close) > period:
                momentum = (close - np.roll(close, period)) / np.roll(close, period)
                momentum[:period] = 0
                features.append(momentum)
            else:
                features.append(np.zeros(len(close)))
        
        # Price position in range (High-Low)
        hl_range = high - low
        hl_range[hl_range == 0] = 1  # Avoid division by zero
        price_position = (close - low) / hl_range
        features.append(price_position)
        
        # Body size (Open-Close) relative to range
        body_size = np.abs(close - open_price) / hl_range
        body_size[hl_range == 0] = 0
        features.append(body_size)
        
        # Upper/Lower shadow
        upper_shadow = (high - np.maximum(close, open_price)) / hl_range
        upper_shadow[hl_range == 0] = 0
        features.append(upper_shadow)
        
        lower_shadow = (np.minimum(close, open_price) - low) / hl_range
        lower_shadow[hl_range == 0] = 0
        features.append(lower_shadow)
        
        # Price distance from moving averages
        for period in [5, 10, 20]:
            if len(close) > period:
                ma = pd.Series(close).rolling(period).mean().values
                distance = (close - ma) / ma
                distance[:period] = 0
                features.append(distance)
            else:
                features.append(np.zeros(len(close)))
        
        # High-Low spread (normalized)
        hl_spread = hl_range / close
        features.append(hl_spread)
        
        return np.column_stack(features)
    
    def _extract_volatility_features(self, data: pd.DataFrame, symbol: str) -> np.ndarray:
        """Extract 100+ volatility features"""
        features = []
        
        close = data['close'].values
        high = data['high'].values
        low = data['low'].values
        
        # Realized Volatility (multiple methods)
        for period in [5, 10, 20]:
            if len(close) > period:
                # Parkinson (high-low) volatility
                hl_vol = np.sqrt((1/(4*np.log(2))) * (np.log(high/low))**2)
                rv_parkinson = pd.Series(hl_vol).rolling(period).mean().values
                rv_parkinson[:period] = 0
                features.append(rv_parkinson)
                
                # Returns-based volatility
                returns = np.diff(np.log(close))
                returns = np.pad(returns, (1, 0), 'constant', constant_values=0)
                rv_returns = pd.Series(returns).rolling(period).std().values * np.sqrt(252*390)  # Annualized
                rv_returns[:period] = 0
                features.append(rv_returns)
            else:
                features.append(np.zeros(len(close)))
                features.append(np.zeros(len(close)))
        
        # VIX features (implied volatility proxy)
        try:
            vix_data = self._get_vix_features()
            if len(vix_data) == len(data):
                features.append(vix_data)
                features.append(vix_data / 20.0 - 1)  # Normalized VIX
            else:
                features.append(np.full(len(close), 20.0))  # Default VIX
                features.append(np.zeros(len(close)))
        except:
            features.append(np.full(len(close), 20.0))
            features.append(np.zeros(len(close)))
        
        # Volatility of volatility
        if len(close) > 20:
            returns = np.diff(np.log(close))
            returns = np.pad(returns, (1, 0), 'constant', constant_values=0)
            vol_of_vol = pd.Series(returns).rolling(10).std().values
            vol_of_vol_ma = pd.Series(vol_of_vol).rolling(10).std().values
            vol_of_vol_ma[:20] = 0
            features.append(vol_of_vol_ma)
        else:
            features.append(np.zeros(len(close)))
        
        # ATR (Average True Range) - multiple periods
        for period in [5, 10, 20]:
            if len(high) > period:
                tr = np.maximum(high - low, 
                               np.maximum(np.abs(high - np.roll(close, 1)),
                                         np.abs(low - np.roll(close, 1))))
                tr[0] = high[0] - low[0]
                atr = pd.Series(tr).rolling(period).mean().values
                atr[:period] = 0
                features.append(atr / close)  # Normalized ATR
            else:
                features.append(np.zeros(len(close)))
        
        # Volatility regimes
        if len(close) > 20:
            returns = np.diff(np.log(close))
            returns = np.pad(returns, (1, 0), 'constant', constant_values=0)
            rv = pd.Series(returns).rolling(20).std().values * np.sqrt(252*390)
            
            # Regime indicators
            low_vol = (rv < 15).astype(float)
            med_vol = ((rv >= 15) & (rv < 25)).astype(float)
            high_vol = (rv >= 25).astype(float)
            
            features.append(low_vol)
            features.append(med_vol)
            features.append(high_vol)
        else:
            features.append(np.zeros(len(close)))
            features.append(np.ones(len(close)))  # Default to medium vol
            features.append(np.zeros(len(close)))
        
        return np.column_stack(features)
    
    def _extract_volume_features(self, data: pd.DataFrame) -> np.ndarray:
        """Extract 50+ volume-based features"""
        features = []
        
        volume = data['volume'].values
        close = data['close'].values
        
        # Volume moving averages
        for period in [5, 10, 20]:
            if len(volume) > period:
                vol_ma = pd.Series(volume).rolling(period).mean().values
                vol_ma[:period] = volume[:period].mean()
                features.append(volume / vol_ma)  # Volume ratio
            else:
                features.append(np.ones(len(volume)))
        
        # Volume-weighted price
        vwap = (data['high'] + data['low'] + data['close']).values / 3
        vwap_volume = (vwap * volume).cumsum() / volume.cumsum()
        features.append((close - vwap_volume) / vwap_volume)  # Price vs VWAP
        
        # Volume trend
        if len(volume) > 10:
            vol_trend = pd.Series(volume).rolling(10).apply(
                lambda x: 1 if x.iloc[-1] > x.iloc[0] else -1
            ).values
            vol_trend[:10] = 0
            features.append(vol_trend)
        else:
            features.append(np.zeros(len(volume)))
        
        # Volume price trend (VPT)
        returns = np.diff(close) / close[:-1]
        returns = np.pad(returns, (1, 0), 'constant', constant_values=0)
        vpt = (volume * returns).cumsum()
        features.append(vpt / vpt.max() if vpt.max() > 0 else np.zeros(len(vpt)))
        
        # On-balance volume (OBV)
        obv = np.zeros(len(close))
        obv[0] = volume[0]
        for i in range(1, len(close)):
            if close[i] > close[i-1]:
                obv[i] = obv[i-1] + volume[i]
            elif close[i] < close[i-1]:
                obv[i] = obv[i-1] - volume[i]
            else:
                obv[i] = obv[i-1]
        features.append(obv / obv.max() if obv.max() > 0 else np.zeros(len(obv)))
        
        # Volume spikes
        if len(volume) > 20:
            vol_mean = pd.Series(volume).rolling(20).mean().values
            vol_std = pd.Series(volume).rolling(20).std().values
            vol_spike = (volume - vol_mean) / (vol_std + 1e-6)
            vol_spike[:20] = 0
            features.append(vol_spike)
        else:
            features.append(np.zeros(len(volume)))
        
        return np.column_stack(features)
    
    def _extract_technical_indicators(self, data: pd.DataFrame) -> np.ndarray:
        """Extract 150+ technical indicators"""
        features = []
        
        close = data['close'].values
        high = data['high'].values
        low = data['low'].values
        volume = data['volume'].values
        
        # RSI (multiple periods)
        for period in [5, 10, 14]:
            if TALIB_AVAILABLE and len(close) > period:
                rsi = talib.RSI(close, timeperiod=period)
                features.append(rsi / 100.0 - 0.5)  # Normalize to [-0.5, 0.5]
            else:
                # Manual RSI calculation
                delta = np.diff(close)
                gain = np.where(delta > 0, delta, 0)
                loss = np.where(delta < 0, -delta, 0)
                avg_gain = pd.Series(gain).rolling(period).mean().values
                avg_loss = pd.Series(loss).rolling(period).mean().values
                rs = avg_gain / (avg_loss + 1e-6)
                rsi = 100 - (100 / (1 + rs))
                rsi = np.pad(rsi, (1, 0), 'constant', constant_values=50)
                rsi[:period] = 50
                features.append(rsi / 100.0 - 0.5)
        
        # MACD
        if len(close) > 26:
            ema12 = pd.Series(close).ewm(span=12).mean().values
            ema26 = pd.Series(close).ewm(span=26).mean().values
            macd = ema12 - ema26
            signal = pd.Series(macd).ewm(span=9).mean().values
            histogram = macd - signal
            
            features.append(macd / close)  # Normalized
            features.append(signal / close)
            features.append(histogram / close)
        else:
            features.append(np.zeros(len(close)))
            features.append(np.zeros(len(close)))
            features.append(np.zeros(len(close)))
        
        # Bollinger Bands
        for period in [10, 20]:
            if len(close) > period:
                ma = pd.Series(close).rolling(period).mean().values
                std = pd.Series(close).rolling(period).std().values
                upper = ma + 2 * std
                lower = ma - 2 * std
                
                features.append((close - ma) / (std + 1e-6))  # Z-score
                features.append((close - lower) / (upper - lower + 1e-6))  # Position in band
            else:
                features.append(np.zeros(len(close)))
                features.append(np.ones(len(close)) * 0.5)
        
        # Moving Averages (multiple types and periods)
        for ma_type in ['SMA', 'EMA']:
            for period in [5, 10, 20, 50]:
                if len(close) > period:
                    if ma_type == 'SMA':
                        ma = pd.Series(close).rolling(period).mean().values
                    else:
                        ma = pd.Series(close).ewm(span=period).mean().values
                    
                    # Distance from MA
                    distance = (close - ma) / ma
                    distance[:period] = 0
                    features.append(distance)
                else:
                    features.append(np.zeros(len(close)))
        
        # Stochastic Oscillator
        if len(high) > 14:
            lowest_low = pd.Series(low).rolling(14).min().values
            highest_high = pd.Series(high).rolling(14).max().values
            stoch = 100 * (close - lowest_low) / (highest_high - lowest_low + 1e-6)
            stoch[:14] = 50
            features.append(stoch / 100.0 - 0.5)  # Normalize
        else:
            features.append(np.zeros(len(close)))
        
        # ADX (Average Directional Index)
        if len(high) > 14:
            # Simplified ADX calculation
            plus_dm = np.maximum(high - np.roll(high, 1), 0)
            minus_dm = np.maximum(np.roll(low, 1) - low, 0)
            tr = np.maximum(high - low,
                           np.maximum(np.abs(high - np.roll(close, 1)),
                                     np.abs(low - np.roll(close, 1))))
            
            plus_di = 100 * pd.Series(plus_dm).rolling(14).mean().values / (pd.Series(tr).rolling(14).mean().values + 1e-6)
            minus_di = 100 * pd.Series(minus_dm).rolling(14).mean().values / (pd.Series(tr).rolling(14).mean().values + 1e-6)
            
            adx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + 1e-6)
            adx[:14] = 0
            features.append(adx / 100.0)
        else:
            features.append(np.zeros(len(close)))
        
        return np.column_stack(features)
    
    def _extract_multi_timescale_features(self, data: pd.DataFrame) -> np.ndarray:
        """Extract 100+ multi-timescale features"""
        features = []
        
        close = data['close'].values
        
        # Resample to multiple timeframes
        data_resampled = data.copy()
        data_resampled.index = pd.date_range(start='2025-01-01', periods=len(data), freq='1min')
        
        for timeframe in [5, 15, 60]:  # 5min, 15min, 1hour
            try:
                resampled = data_resampled.resample(f'{timeframe}min').agg({
                    'open': 'first',
                    'high': 'max',
                    'low': 'min',
                    'close': 'last',
                    'volume': 'sum'
                }).dropna()
                
                if len(resampled) > 0:
                    # Forward fill to match original length
                    resampled_close = resampled['close'].reindex(
                        data_resampled.index, method='ffill'
                    ).values
                    
                    # Returns at this timeframe
                    returns = np.diff(resampled_close) / resampled_close[:-1]
                    returns = np.pad(returns, (1, 0), 'constant', constant_values=0)
                    
                    features.append(returns)
                    
                    # Moving average distance
                    if len(resampled_close) > 20:
                        ma = pd.Series(resampled_close).rolling(20).mean().values
                        distance = (resampled_close - ma) / ma
                        features.append(distance)
                    else:
                        features.append(np.zeros(len(close)))
                else:
                    features.append(np.zeros(len(close)))
                    features.append(np.zeros(len(close)))
            except:
                features.append(np.zeros(len(close)))
                features.append(np.zeros(len(close)))
        
        return np.column_stack(features) if features else np.zeros((len(close), 1))
    
    def _extract_cross_asset_features(self, symbol: str, data: pd.DataFrame) -> np.ndarray:
        """Extract 50+ cross-asset features"""
        features = []
        
        close = data['close'].values
        
        # VIX features
        try:
            vix = self._get_vix_features()
            if len(vix) == len(close):
                features.append(vix / 20.0 - 1)  # Normalized
                
                # VIX vs price correlation (rolling)
                if len(close) > 20:
                    corr = pd.Series(close).rolling(20).corr(pd.Series(vix)).values
                    corr[:20] = 0
                    features.append(corr)
                else:
                    features.append(np.zeros(len(close)))
            else:
                features.append(np.zeros(len(close)))
                features.append(np.zeros(len(close)))
        except:
            features.append(np.zeros(len(close)))
            features.append(np.zeros(len(close)))
        
        # SPX/QQQ correlation if not trading SPY
        if symbol != 'SPY':
            try:
                spy_data = yf.download('SPY', period='1d', interval='1m', progress=False)
                if isinstance(spy_data.columns, pd.MultiIndex):
                    spy_data.columns = spy_data.columns.get_level_values(0)
                spy_close = spy_data['Close'].tail(len(close)).values
                
                if len(spy_close) == len(close):
                    # Correlation
                    if len(close) > 20:
                        corr = pd.Series(close).rolling(20).corr(pd.Series(spy_close)).values
                        corr[:20] = 0
                        features.append(corr)
                    else:
                        features.append(np.zeros(len(close)))
                    
                    # Relative strength
                    rel_strength = (close / close[0]) / (spy_close / spy_close[0]) - 1
                    features.append(rel_strength)
                else:
                    features.append(np.zeros(len(close)))
                    features.append(np.zeros(len(close)))
            except:
                features.append(np.zeros(len(close)))
                features.append(np.zeros(len(close)))
        
        return np.column_stack(features) if features else np.zeros((len(close), 1))
    
    def _extract_microstructure_features(self, data: pd.DataFrame) -> np.ndarray:
        """Extract 50+ market microstructure features"""
        features = []
        
        high = data['high'].values
        low = data['low'].values
        close = data['close'].values
        volume = data['volume'].values
        
        # Bid-ask spread proxy (using high-low)
        spread = high - low
        spread_norm = spread / close
        features.append(spread_norm)
        
        # Price impact proxy
        returns = np.abs(np.diff(close) / close[:-1])
        returns = np.pad(returns, (1, 0), 'constant', constant_values=0)
        volume_norm = volume / (volume.max() + 1e-6)
        price_impact = returns / (volume_norm + 1e-6)
        features.append(price_impact)
        
        # Order flow imbalance proxy
        body = close - data['open'].values
        order_flow = body * volume
        features.append(order_flow / (order_flow.max() + 1e-6))
        
        # Fill the rest with zeros (would need Level 2 data for real microstructure)
        remaining_features = 50 - len(features)
        for _ in range(remaining_features):
            features.append(np.zeros(len(close)))
        
        return np.column_stack(features)
    
    def _extract_position_features(self, data: pd.DataFrame, risk_mgr) -> np.ndarray:
        """Extract 10+ position and risk features"""
        features = []
        
        close = data['close'].values
        
        # Position information
        has_position = 1.0 if risk_mgr.open_positions else 0.0
        features.append(np.full(len(close), has_position))
        
        # Number of positions
        num_positions = len(risk_mgr.open_positions) / 10.0  # Normalize
        features.append(np.full(len(close), num_positions))
        
        # Total exposure (normalized)
        total_exposure = risk_mgr.get_current_exposure()
        max_exposure = 50000.0  # Assume max
        exposure_ratio = total_exposure / (max_exposure + 1e-6)
        features.append(np.full(len(close), exposure_ratio))
        
        # Daily P&L
        daily_pnl = risk_mgr.daily_pnl
        features.append(np.full(len(close), daily_pnl))
        
        # Fill remaining with zeros
        remaining = 10 - len(features)
        for _ in range(remaining):
            features.append(np.zeros(len(close)))
        
        return np.column_stack(features)
    
    def _get_vix_features(self) -> np.ndarray:
        """Get VIX features"""
        try:
            vix = yf.Ticker("^VIX")
            vix_data = vix.history(period="1d", interval="1m")
            if isinstance(vix_data.columns, pd.MultiIndex):
                vix_data.columns = vix_data.columns.get_level_values(0)
            
            if len(vix_data) > 0:
                return vix_data['Close'].values
            else:
                return np.array([20.0])
        except:
            return np.array([20.0])
    
    def get_feature_count(self) -> int:
        """Get total number of features"""
        # Approximate counts
        return 500  # Adjust based on actual feature extraction


# Factory function for easy integration
def create_feature_engine(lookback_minutes: int = 20) -> InstitutionalFeatureEngine:
    """Create and return a configured feature engine"""
    return InstitutionalFeatureEngine(lookback_minutes=lookback_minutes)

